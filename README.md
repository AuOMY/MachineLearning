# Linear regression
## 一元线性回归 Simple linear regression
1、给定一组数据(1, 9); (1.1, 10.5); (2, 18); (3, 28); (3.2, 30); (4, 37); (5, 48); (1.2, 10);   
试用一元线性回归预测：当x取值为3.5 和 4时，y的值。  
注：需要用闭式解和梯度下降两者方法，给出相应代码；对比两者之间的误差。 

**闭式解**  
闭式解，即解析解。对于某方程组的求解，根据求解方式的不同，可以分为两种解：解析解与数值解。解析解通俗地讲，是利用微积分、基本函数等“公式化”、“套路化”的方式求解得出的解，也可称为闭式解；在工程中存在许多无法得出解析解的问题，此时可通过数值算法，如插值、拟合等方法求出问题的数值解。   

一元线性回归中试图学习得到 **F(x) = wx + b** 形式的函数，使得 |F(xi) - yi| 无限趋近于0，对于回归问题而言，均方误差是最常用的性能度量，为了使函数尽可能符合，需要试图令均方误差最小化。    

均方误差 E(f;D) = ∑(f(xi) - yi)^2 / m ， 带入F(x)形式的函数可得 E(f;D) = ∑(wxi + b - yi)^2 / m ，由于求解 w 和 b 仅与分母函数有关，故 (w*,b*) = arg min ∑(wxi + b - yi)^2。  
注：根据简单的数学关系，在同一问题中，均方误差同乘同除并不会影响相对的误差比较，而要求解最值问题，往往是利用微积分知识，涉及导数，于是可以将其均方误差(也可叫做损失函数)设为 E(f;D) = ∑(f(xi) - yi)^2 / (2 * m) ，简化计算。  

均方误差具有非常好的几何意义，对应着欧几里得距离，利用该几何意义进行均方误差的最小化，即得到“最小二乘法”模型。  

利用微积分的知识，最值问题的解往往在边界点与极值点，对此，我们往往先入为主地认为极值就是最值，但“显而易见”是显而易见地缺乏可信性的。没关系，可以从我们所得到的目标函数 G(w，b) = ∑(f(xi) - yi)^2 出发，其为二次函数形式，在机器学习中，我们认为 G(w，b) 是关于 w 和 b 的凸函数，凸函数是拥有局部最小值或全局最小值的，目标函数显然是简单的凸函数，拥有全局最小值，根据性质知全局最小值即为极值。  

凸函数：  
![](https://github.com/AuOMY/MachineLearning/blob/Linear-regression/%E5%87%B8%E5%87%BD%E6%95%B0.png?raw=true)  

接下来只需要对 w 和 b 求导解得极值即可，得 w = ∑yi(xi - (∑xi / m)) / (∑xi^2 - (∑xi)^2 / m)，b = ∑(yi - wxi) / m 。
